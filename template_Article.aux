\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problem Statement}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The BLEU Metric}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Benchmark Model}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}The Datasets}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Machine Translation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Word Embeddings}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Recurrent Neural Networks}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}LSTMs vs GRUs}{4}}
\newlabel{eq:hiddenLSTM}{{6}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic of encoder-decoder architecture. In this instance, the decoder is fed in previously generated tokens as input. Figure from [18]}}{5}}
\newlabel{fig:encoder-decoder}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Encoder-Decoder Architecture}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analysis}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data Exploration}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Algorithms}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data Preprocessing}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Implementation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Refinement}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Model Evaluation and Validation}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Improvement}{8}}
